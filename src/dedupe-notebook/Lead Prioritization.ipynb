{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "fe95640e-3a7c-47ff-b62c-90c6607eaadb",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "SELECT * FROM `hive_metastore`.`default`.`ice_data_2025_02_28_135500_2_csv` where is_dupe = \"False\";"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3d2ffc82-267f-437e-abf4-9dcbfb8afa9f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "df = spark.table(\"hive_metastore.default.ice_data_2025_02_28_135500_2_csv\").toPandas()\n",
    "df.head()\n",
    "df_original = df\n",
    "\n",
    "# filter to just is_dupe = False\n",
    "df = df[df['is_dupe'] == 'False']\n",
    "\n",
    "df.info()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "024f5ecb-21c2-4449-bb4c-3b43e9a129a9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# load the airports GIS data\n",
    "df_airports = spark.table(\"default.international_airports_lower_48\").toPandas()\n",
    "df_airports.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "84c4e7f7-2b9f-4d25-a62a-cb84a68adaff",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from math import radians, cos, sin, asin, sqrt\n",
    "\n",
    "# add a haversine distance formula\n",
    "def haversine(lon1, lat1, lon2, lat2):\n",
    "    lon1, lat1, lon2, lat2 = map(radians, [lon1, lat1, lon2, lat2])\n",
    "    dlon = lon2 - lon1\n",
    "    dlat = lat2 - lat1\n",
    "    a = sin(dlat/2)**2 + cos(lat1) * cos(lat2) * sin(dlon/2)**2\n",
    "    c = 2 * asin(sqrt(a))\n",
    "    r = 6371\n",
    "    # return distance in miles\n",
    "    miles = c * r * 0.621371\n",
    "    return miles\n",
    "\n",
    "# find the closest airport for each ice lead\n",
    "def find_closest_airport(row):\n",
    "    min_distance = float('inf')\n",
    "    closest_airport = None\n",
    "    for _, airport in df_airports.iterrows():\n",
    "        distance = haversine(float(row['longitude']), float(row['latitude']), float(airport['Longitude']), float(airport['Latitude']))\n",
    "        if distance < min_distance:\n",
    "            min_distance = distance\n",
    "            closest_airport = airport['Airport Name']\n",
    "    return pd.Series([closest_airport, min_distance], index=['closest_airport', 'distance_to_airport'])\n",
    "\n",
    "df[['closest_airport', 'distance_to_airport']] = df.apply(find_closest_airport, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0f6b01cc-19bb-4c08-bc0f-e14a4c318b22",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "df.head()\n",
    "\n",
    "# For verfication, select just the lead_id, address, closest airport, and distance for rows with an address\n",
    "verification_df = df[['lead_id', 'last_known_address', 'closest_airport', 'distance_to_airport']][df['last_known_address'].notnull()]\n",
    "\n",
    "# show the top 10 leads closest to an airport\n",
    "closest_df = verification_df.sort_values(by='distance_to_airport')\n",
    "display(closest_df.head(10))\n",
    "\n",
    "# show the top 10 furthest leads from an airport\n",
    "furthest_df = verification_df.sort_values(by='distance_to_airport', ascending=False)\n",
    "display(furthest_df.head(10))\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "42bd1206-267f-4fbc-8d11-77614284114d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# load the flights table into a df\n",
    "df_flights = spark.table(\"default.test_flight_data\").toPandas()\n",
    "\n",
    "display(df_flights)\n",
    "\n",
    "\n",
    "\n",
    "display(df_flights.groupby('Destination')['Available Seats'].sum().sort_values(ascending=False))\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "# Create a plot for each Destination and show a distribution of the available seats\n",
    "for destination, group in df_flights.groupby('Destination'):\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.hist(group['Available Seats'], bins=20, edgecolor='k', alpha=0.7)\n",
    "    plt.title(f'Distribution of Available Seats for {destination}')\n",
    "    plt.xlabel('Available Seats')\n",
    "    plt.ylabel('Frequency')\n",
    "    \n",
    "    # Calculate quartiles\n",
    "    q1 = group['Available Seats'].quantile(0.25)\n",
    "    q2 = group['Available Seats'].quantile(0.5)\n",
    "    q3 = group['Available Seats'].quantile(0.75)\n",
    "    \n",
    "    # Add quartile markings\n",
    "    plt.axvline(q1, color='r', linestyle='dashed', linewidth=1, label='Q1')\n",
    "    plt.axvline(q2, color='r', linestyle='dashed', linewidth=1, label='Q2')\n",
    "    plt.axvline(q3, color='r', linestyle='dashed', linewidth=1, label='Q3')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "    \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7efe543a-7556-45f0-a568-df906bc26323",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# create some weighted prioritization functions\n",
    "\n",
    "# lead needs to have an address\n",
    "def address_weight(row):\n",
    "    return 5 if pd.notnull(row['last_known_address']) else 0\n",
    "\n",
    "def risk_weight(row):\n",
    "    if row['risk_level'] == 'High':\n",
    "        return 5\n",
    "    if row['risk_level'] == 'Medium':\n",
    "        return 3\n",
    "    return 1\n",
    "\n",
    "def airport_weight(row):\n",
    "    if row['distance_to_airport'] < 100:\n",
    "        return 5\n",
    "    if row['distance_to_airport'] < 250:\n",
    "        return 2\n",
    "    return 0\n",
    "\n",
    "def available_seats_for_destination_country_for_closest_airport_weight(row):\n",
    "    destination_country = row['country_of_origin']\n",
    "    closest_airport = row['closest_airport']\n",
    "    \n",
    "    # Filter flights to the closest airport and destination country\n",
    "    relevant_flights = df_flights[(df_flights['Destination'] == destination_country) & \n",
    "                                  (df_flights['Airport Name'] == closest_airport)]\n",
    "    \n",
    "    # Sum available seats\n",
    "    total_available_seats = relevant_flights['Available Seats'].sum()\n",
    "    \n",
    "    # Assign weight based on available seats\n",
    "    if total_available_seats > 30:\n",
    "        return 5\n",
    "    elif total_available_seats > 20:\n",
    "        return 3\n",
    "    elif total_available_seats > 10:\n",
    "        return 1\n",
    "    else:\n",
    "        return 0\n",
    "\n",
    "\n",
    "def organized_crime_weight(row):\n",
    "    # if pd null return 0, otherwise 5\n",
    "    if pd.notnull(row['organized_crime_links']):\n",
    "        return 5\n",
    "    return 0\n",
    "\n",
    "def country_of_origin_is_central_america(row): \n",
    "    # Mexico, Guatemela, Hondurus deportation agreements \n",
    "    if row['country_of_origin'] in ['Mexico', 'Guatemala', 'Honduras']:\n",
    "        return 5\n",
    "    if row['country_of_origin'] == 'India':\n",
    "        return 1\n",
    "    return 0\n",
    "    \n",
    "\n",
    "def prioritize(row):\n",
    "    return (address_weight(row) + risk_weight(row) + airport_weight(row) + organized_crime_weight(row) + country_of_origin_is_central_america(row)) + available_seats_for_destination_country_for_closest_airport_weight(row)\n",
    "\n",
    "# add a \"lead_prioritization number\" column with the weighted score separate from the lead prioritization field\n",
    "df['lead_prioritization_number'] = df.apply(prioritize, axis=1)\n",
    "\n",
    "# create a validation df with the columns used for weighting, the lead id, and the weighted score\n",
    "verification_df = df[['lead_prioritization_number', 'lead_id', 'last_known_address', 'risk_level', 'distance_to_airport', 'organized_crime_links', 'country_of_origin' ]]\n",
    "\n",
    "# filter to the top 5 scores and diplay\n",
    "display(verification_df.sort_values(by='lead_prioritization_number', ascending=False).head(5))\n",
    "\n",
    "# filter to the bottom 5 scores and diplay\n",
    "display(verification_df.sort_values(by='lead_prioritization_number').head(5))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1dcddc02-5942-4e42-a966-4c3be50591c8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# Calculate quartiles\n",
    "quartiles = np.percentile(df['lead_prioritization_number'], [25, 50, 75])\n",
    "\n",
    "# Create a figure and axis\n",
    "plt.figure(figsize=(10, 6))\n",
    "\n",
    "# Plot histogram\n",
    "plt.hist(df['lead_prioritization_number'], bins=20, edgecolor='k', alpha=0.7)\n",
    "\n",
    "# Add vertical lines for quartiles\n",
    "for quartile in quartiles:\n",
    "    plt.axvline(quartile, color='r', linestyle='dashed', linewidth=1)\n",
    "\n",
    "# Add text for quartiles\n",
    "for i, quartile in enumerate(quartiles):\n",
    "    plt.text(quartile, plt.ylim()[1] * 0.9, f'Q{i+1}', color='r', ha='center')\n",
    "\n",
    "# Add titles and labels\n",
    "plt.title('Distribution of Lead Prioritization Scores with Quartiles')\n",
    "plt.xlabel('Lead Prioritization Score')\n",
    "plt.ylabel('Frequency')\n",
    "plt.grid(True)\n",
    "\n",
    "# Show plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "38a437a9-96fe-4cba-8c29-825b2097e756",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%python\n",
    "# count unique lead_ids\n",
    "rows, columns  = df_original.shape\n",
    "\n",
    "def include_risk(row):\n",
    "    if row['risk_level'] == 'High':\n",
    "        return True\n",
    "    if row['risk_level'] == 'Medium':\n",
    "        return True\n",
    "    return False\n",
    "\n",
    "def include_organized_crime(row):\n",
    "    if row['organized_crime_links'] is not None:\n",
    "        return True\n",
    "    return False\n",
    "\n",
    "def include_close_to_airport(row):\n",
    "    if row['distance_to_airport'] < 100:\n",
    "        return True\n",
    "    return False\n",
    "\n",
    "key_info = {}\n",
    "data = \"Total â¬›ï¸ \"\n",
    "key_info[0] = {\"data\": data , \"count\": rows}\n",
    "\n",
    "# filter is_dupe = False\n",
    "rows, columns = df_original[df_original['is_dupe'] == \"False\"].shape   \n",
    "data = \"Without duplicates ðŸ”€\"\n",
    "key_info[1] = {\"data\": data , \"count\": rows}\n",
    "\n",
    "# use the include risk function to filter rows\n",
    "rows, columns = df_original[\n",
    "    (df_original['is_dupe'] == \"False\") & \n",
    "    (df_original.apply(include_risk, axis=1))\n",
    "].shape\n",
    "data = \"High/Medium risk ðŸš©\"\n",
    "key_info[2] = {\"data\": data , \"count\": rows}\n",
    "\n",
    "# filter by organized crime\n",
    "rows, columns = df_original[\n",
    "    (df_original['is_dupe'] == \"False\") & \n",
    "    (df_original.apply(include_organized_crime, axis=1))\n",
    "].shape\n",
    "data = \"Organized crime âš–ï¸\"\n",
    "key_info[3] = {\"data\": data , \"count\": rows}\n",
    "\n",
    "# filter by close to airport\n",
    "rows, columns = df[\n",
    "    (df['is_dupe'] == \"False\") & \n",
    "    (df.apply(include_close_to_airport, axis=1))\n",
    "].shape\n",
    "data = \"Close to airport âœˆï¸\"\n",
    "key_info[4] = {\"data\": data , \"count\": rows}\n",
    "\n",
    "# filter is dupe = false, shorter flights based on country of origin is Mexico, Guatamala, Hondurus\n",
    "rows, columns = df_original[\n",
    "    (df_original['is_dupe'] == \"False\") & \n",
    "    (df_original['country_of_origin'].isin(['Mexico', 'Guatemala', 'Honduras']))\n",
    "].shape\n",
    "data = \"Shorter flights ðŸŒŽ\"\n",
    "key_info[6] = {\"data\": data , \"count\": rows}\n",
    "\n",
    "# convert key info into a pandas df where the key is th\n",
    "df_key_info = pd.DataFrame.from_dict(key_info, orient='index', columns=['data', \"count\" ])\n",
    "# add thousands separator to count\n",
    "df_key_info['count'] = df_key_info['count'].apply(lambda x: \"{:,}\".format(x))\n",
    "\n",
    "display(df_key_info)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "98c5671a-f182-4dbd-89e4-fdddfdbbfc87",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# update the lead_prioritization field to be High is score > 18, Medium if 18 >= score > 15, and Low if score <= 15\n",
    "df['lead_prioritization'] = df['lead_prioritization_number'].apply(lambda x: 'High' if x > 18 else ('Medium' if x > 15 else 'Low'))\n",
    "\n",
    "\n",
    "# show a pie chart of the lead_prioritization field use a color scheme where High = red, Medium = yellow, and Low = green\n",
    "colors = {'High': 'red', 'Medium': 'yellow', 'Low': 'green'}\n",
    "plt.figure(figsize=(6, 6))\n",
    "plt.pie(df['lead_prioritization'].value_counts(), labels=df['lead_prioritization'].value_counts().index, colors=df['lead_prioritization'].map(colors))\n",
    "plt.title('Lead Prioritization Distribution')\n",
    "plt.show()\n",
    "\n",
    " \n"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 140516381327372,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "Lead Prioritization",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
